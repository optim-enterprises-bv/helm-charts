OptimACS has been deployed.

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
 Prerequisites (must exist before helm install)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

1. TLS Secret ({{ .Values.tlsSecret.name }}):

   kubectl create secret generic {{ .Values.tlsSecret.name }} \
     --from-file=server.crt=/path/to/server.crt  \
     --from-file=server.key=/path/to/server.key  \
     --from-file=rootCA.crt=/path/to/rootCA.crt  \
     --from-file=rootCA.key=/path/to/rootCA.key  \
     --namespace {{ .Release.Namespace }}

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
 MySQL cluster
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
{{- if .Values.mysql.enabled }}

Architecture : {{ .Values.mysql.architecture }}
Primary host : {{ include "optimacs.mysql.host" . }}.{{ .Release.Namespace }}.svc.cluster.local:3306
Database     : {{ include "optimacs.mysql.database" . }}
User         : {{ include "optimacs.mysql.user" . }}

The schema is created automatically via initdbScripts on first boot.
Wait for the primary pod to be Ready before the ac-server and UI pods
will pass their readiness probes:

  kubectl get pods -l app.kubernetes.io/name=mysql \
    --namespace {{ .Release.Namespace }} --watch

Create the first admin user (run once after the UI pod is Ready):

  kubectl exec -n {{ .Release.Namespace }} \
    $(kubectl get pod -n {{ .Release.Namespace }} -l "{{ include "optimacs.ui.selectorLabels" . }}" -o name | head -1) \
    -- python create_admin.py --username admin \
                              --password <choose-a-password> \
                              --role full_admin
{{- else }}
  mysql.enabled=false — connecting to external host: {{ .Values.config.dbHost }}
{{- end }}

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
 Connect to the ACP server
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
{{- if eq .Values.service.type "LoadBalancer" }}
  Wait for an external IP:
    kubectl get svc {{ include "ac-server.fullname" . }} \
      --namespace {{ .Release.Namespace }} --watch

  Access Point devices should connect to:
    <EXTERNAL-IP>:{{ .Values.service.port }}
{{- else if eq .Values.service.type "NodePort" }}
  Access Point devices should connect to any node IP on port:
    {{ .Values.service.nodePort | default "see `kubectl get svc`" }}
{{- else }}
  Service type is {{ .Values.service.type }}. Expose port {{ .Values.service.port }} as needed.
{{- end }}

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
 Web UI
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
{{- if .Values.ui.enabled }}
{{- if .Values.ui.ingress.enabled }}
  Management UI:  http{{ if .Values.ui.ingress.tls }}s{{ end }}://{{ .Values.ui.ingress.hostname }}
{{- else }}
  Port-forward to access the UI locally:
    kubectl port-forward svc/{{ include "optimacs.ui.fullname" . }} 8080:{{ .Values.ui.service.port }} \
      --namespace {{ .Release.Namespace }}

  Then open: http://localhost:8080
{{- end }}

  GraphQL playground: <ui-url>/graphql
{{- else }}
  ui.enabled=false — web UI not deployed.
{{- end }}

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
 Redis cluster
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
{{- if .Values.redis.enabled }}

Architecture : {{ .Values.redis.architecture }} (1 primary + {{ .Values.redis.replica.replicaCount }} replicas + Sentinel)
Primary svc  : {{ .Release.Name }}-redis-master:6379
Sentinel svc : {{ .Release.Name }}-redis:26379

Redis Sentinel provides automatic primary failover.  The master service
selector is updated by Sentinel so ac-server reconnects transparently.

Check cluster status:
  kubectl exec -n {{ .Release.Namespace }} \
    $(kubectl get pod -n {{ .Release.Namespace }} -l "app.kubernetes.io/name=redis,app.kubernetes.io/component=master" -o name | head -1) \
    -- redis-cli info replication
{{- else }}
  redis.enabled=false — Redis cache/registry disabled.
  Enable with: --set redis.enabled=true
{{- end }}

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
 Telemetry pipeline
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
{{- if or .Values.redpanda.enabled .Values.redpanda.externalBrokers }}

Redpanda brokers : {{ include "optimacs.telemetry.redpanda.brokers" . }}
Topics:
  optimacs.heartbeat     — ValueChange parameter events    (4 partitions)
  optimacs.device.status — Boot! / OnBoardRequest events   (2 partitions)
  optimacs.config.change — config-push confirmations       (2 partitions)

Topics are created automatically by the redpanda-init Job on first start.
{{- else }}

  Telemetry disabled — set redpanda.enabled=true or redpanda.externalBrokers
  to start publishing USP events to Redpanda.
{{- end }}
{{- if .Values.vector.enabled }}
{{- $vmUrl := include "optimacs.telemetry.victoriametrics.url" . }}
{{- if $vmUrl }}

Vector pipeline : running
  Route:   Redpanda topics → log_to_metric → VictoriaMetrics (prometheus_remote_write)
  Remote-write endpoint: {{ $vmUrl }}

  Metrics written:
    optimacs_heartbeat_gauge{mac,endpoint_id,param_path}   — USP ValueChange values
    optimacs_device_events_total{mac,endpoint_id,event}    — Boot!/OnBoard counters
    optimacs_config_changes_total{mac,endpoint_id}         — config-push counters

  Check Vector logs:
    kubectl logs -l app.kubernetes.io/component=vector \
      --namespace {{ .Release.Namespace }} --tail=50

  Dashboard ConfigMap "{{ include "ac-server.fullname" . }}-grafana-dashboards"
  carries label {{ .Values.victoriametrics.grafana.dashboardLabel }}={{ .Values.victoriametrics.grafana.dashboardLabelValue }}
  so the Grafana sidecar auto-loads it.
  Datasource UID used: {{ .Values.victoriametrics.grafana.datasourceUid }}
  (override with --set victoriametrics.grafana.datasourceUid=<your-uid> if needed)
{{- else }}

  vector.enabled=true but victoriametrics.remoteWriteUrl is not set.
  Set it to the /api/v1/write endpoint of your VictoriaMetrics instance:
    --set victoriametrics.remoteWriteUrl=http://victoria-metrics.<ns>.svc:8428/api/v1/write
{{- end }}
{{- end }}

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
 Databunker PII vault
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
{{- if .Values.databunker.enabled }}

  Databunker is deployed and stores all PII (customer profiles, billing
  addresses, payment tokens) with AES-256 per-record encryption, isolated
  from the operational MySQL database.

  Internal service URL (used by optimacs-ui automatically):
    http://{{ include "optimacs.databunker.fullname" . }}:{{ .Values.databunker.port }}

  ⚠ DATABUNKER_MASTERKEY is stored in:
      kubectl get secret {{ include "optimacs.databunker.secretName" . }} \
        -n {{ .Release.Namespace }}

    LOSING THIS KEY MAKES ALL PII RECORDS PERMANENTLY UNRECOVERABLE.
    Back it up to a secure secrets manager (Vault, AWS Secrets Manager, etc.)
    immediately.

{{- else }}

  databunker.enabled=false — the UI will connect to the external Databunker
  instance at: {{ .Values.databunker.url | default "(databunker.url not set)" }}

  Ensure databunker.rootToken is set correctly so DATABUNKER_ROOT_TOKEN is
  injected into the UI Secret.

{{- end }}

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
 Security notes
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
• TLS 1.3 only — hybrid X25519+ML-KEM-768 key exchange (NIST FIPS 203).
• DB password stored in a Kubernetes Secret, not a ConfigMap.
• Containers run as non-root (UID 1000).
• ac-server has a read-only root filesystem.
• PII vault (Databunker) uses AES-256 per-record encryption; operational DB
  holds only a numeric tenant ID — a DB dump contains no plaintext PII.
